# Import necessary libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# Step 1: Load data (Assume DataFrame `df` has the required columns)
# Example of loading a CSV file
# df = pd.read_csv('employee_salaries.csv')

# For illustration, let's create a sample dataset:
data = {
    'YearsExperience': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
    'Salary': [40000, 45000, 50000, 60000, 65000, 70000, 75000, 80000, 85000, 90000]
}

df = pd.DataFrame(data)

# Step 2: Split the data into features (X) and target (y)
X = df[['YearsExperience']].values  # Features: Years of experience
y = df['Salary'].values  # Target: Salary

# Step 3: Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 4: Create polynomial features (degree 2 or higher for non-linearity)
poly = PolynomialFeatures(degree=3)  # You can change the degree as needed
X_poly_train = poly.fit_transform(X_train)
X_poly_test = poly.transform(X_test)

# Step 5: Fit the Polynomial Regression model
model = LinearRegression()
model.fit(X_poly_train, y_train)

# Step 6: Make predictions
y_pred = model.predict(X_poly_test)

# Step 7: Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse}")

# Visualize the result (to compare linear vs polynomial fit)
plt.scatter(X, y, color='red')  # Original data
plt.plot(X, model.predict(poly.transform(X)), color='blue')  # Polynomial regression fit
plt.title('Polynomial Regression (Degree 3) for Salary Prediction')
plt.xlabel('Years of Experience')
plt.ylabel('Salary')
plt.show()

