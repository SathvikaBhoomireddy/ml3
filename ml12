# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score, roc_auc_score
import matplotlib.pyplot as plt
import seaborn as sns

# Load and inspect data
data = pd.read_csv("telecom_churn.csv")
print(data.head())

# EDA and Data Cleaning
# Visualize churn distribution
sns.countplot(x='churn', data=data)
plt.show()

# Data preprocessing
# Handle missing values, outliers, and encode categorical variables
data.fillna(data.median(), inplace=True)
data = pd.get_dummies(data, drop_first=True)

# Feature engineering (example)
data['total_services'] = data[['internet_service', 'phone_service', 'tv_service']].sum(axis=1)
data['contract_duration_years'] = data['tenure'] / 12

# Split data
X = data.drop(columns=['churn'])
y = data['churn']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Model selection
# Logistic Regression
logreg = LogisticRegression(max_iter=1000)
logreg.fit(X_train, y_train)
y_pred_logreg = logreg.predict(X_test)
print("Logistic Regression Classification Report:\n", classification_report(y_test, y_pred_logreg))

# Random Forest with cross-validation and hyperparameter tuning
rf = RandomForestClassifier()
param_grid = {'n_estimators': [100, 200], 'max_depth': [10, 20]}
grid_search = GridSearchCV(rf, param_grid, cv=5)
grid_search.fit(X_train, y_train)
y_pred_rf = grid_search.best_estimator_.predict(X_test)
print("Random Forest Classification Report:\n", classification_report(y_test, y_pred_rf))

# Evaluate feature importance (Random Forest)
importances = grid_search.best_estimator_.feature_importances_
feature_importance = pd.Series(importances, index=X.columns).sort_values(ascending=False)
feature_importance.plot(kind='bar')
plt.show()
